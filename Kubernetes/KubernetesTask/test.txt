task 7 = 
a. Create a deployment deploy01 with the image nginx and port 80 exposed:
kubectl create deployment deploy01 --image=nginx --port=80

b. Update the deployment deploy01 with the image nginx:dev and record the execution:
kubectl set image deployment/deploy01 nginx=nginx:dev --record=true

c. If the deployment in step b is unsuccessful, rollback the deployment to the previous state:
kubectl rollout undo deployment/deploy01

d. If the Nginx deployment is in a healthy state, scale the deployment to run 3 replicas and record the execution:
kubectl scale deployment/deploy01 --replicas=3 --record=true

e. Display the history of deployment deploy01 and store it in a file named /opt/answers/31.txt:
kubectl rollout history deployment/deploy01 > /opt/answers/31.txt

These commands will help you create, update, rollback, scale, and record the history of the deploy01 deployment as per your requirements.

----------------------------------------------------------


apiVersion:

Subfield: Specifies the API version for the Kubernetes resource.
Usage: Indicates which version of the Kubernetes API the resource adheres to. Different API versions may have varying features and capabilities.

kind:
Subfield: Specifies the type of Kubernetes resource being defined (e.g., Deployment, Pod, Service).
Usage: Identifies the type of resource the YAML file describes, guiding how Kubernetes should treat it.

metadata:
Subfields: name, namespace, labels, annotations, and more.
Usage: Contains metadata about the resource, such as its name, namespace (optional), labels (key-value pairs for categorization), and annotations (additional information).

spec:
Subfields: Varies depending on the resource type (e.g., containers, volumes, replicas, ports, etc.).
Usage: Contains the desired state and configuration for the resource. The subfields within spec are specific to the resource type.

containers:
Subfields: name, image, ports, command, args, and more.
Usage: Describes the containers to run within a Pod. You specify the container name, the Docker image to use, ports to expose, and optionally, the command and arguments for the container.

volumes:
Subfields: name, persistentVolumeClaim, emptyDir, hostPath, and more.
Usage: Defines storage volumes that can be mounted into containers. These volumes can be used to store and share data between containers and across restarts.

replicas:
Subfield: Specifies the desired number of replicas for a resource (e.g., Deployment, ReplicaSet).
Usage: Indicates how many instances of a resource, like Pods, should be maintained. Useful for achieving high availability and load balancing.

ports:
Subfields: containerPort, name, protocol, targetPort, and more.
Usage: Defines the network ports to expose for communication. It includes details like the container port, target port, and protocol (e.g., TCP or UDP).

selector:
Subfield: Specifies the criteria for selecting Pods (e.g., labels).
Usage: Used in resources like Services and Deployments to determine which Pods to include or route traffic to based on their labels.

strategy:
Subfields: type, rollingUpdate, and more (used in Deployments).
Usage: Configures the deployment strategy, including rolling updates, to control how new versions of an application are rolled out.

serviceType:
Subfield: Specifies the type of Service (e.g., ClusterIP, NodePort, LoadBalancer).
Usage: Determines how the Service should be exposed outside the cluster or within the cluster network.

volumeClaimTemplates:
Subfields: metadata and spec.
Usage: Used in StatefulSets to define how Persistent Volume Claims (PVCs) should be created and bound to Pods.
These fields and subfields are the building blocks of Kubernetes resource configurations. Depending on the specific resource you're defining (e.g., Pod, Deployment, Service), you'll use these fields to describe the desired state of your application or infrastructure in the cluster.


-------------------------------------------------------
task = 8


To create a pod that uses secrets in Kubernetes, you can configure secrets in two different ways:

a. Pull secrets from environment variables:

First, create a secret containing your sensitive data. For example, let's create a secret named "my-secret" with a key "password":

kubectl create secret generic my-secret --from-literal=password=supersecret

Now, create a pod that uses this secret in environment variables:

apiVersion: v1
kind: Pod
metadata:
  name: secret-env-pod
spec:
  containers:
  - name: secret-env-container
    image: nginx
    env:
    - name: MY_PASSWORD
      valueFrom:
        secretKeyRef:
          name: my-secret
          key: password


b. Pull secrets from a volume:
You can create a secret and mount it as a volume in a pod. Here's an example:

apiVersion: v1
kind: Pod
metadata:
  name: secret-volume-pod
spec:
  containers:
  - name: secret-volume-container
    image: nginx
    volumeMounts:
    - name: secret-volume
      mountPath: /etc/secrets
  volumes:
  - name: secret-volume
    secret:
      secretName: my-secret


In both cases, the secret "my-secret" is being used, either as environment variables (MY_PASSWORD) or mounted as a volume at /etc/secrets.

c. Dump the secrets using kubectl:

You can use kubectl to check that your secrets are available in the pod. For example, to check the environment variable in the first pod:
kubectl exec secret-env-pod -- env | grep MY_PASSWORD

And to check the content of the mounted volume in the second pod:
kubectl exec secret-volume-pod -- cat /etc/secrets/password

These commands will show that the secrets are accessible in the respective pods


------------------------------------------------------------


Certainly! Let's walk through how Kubernetes (K8s) architecture works when you create a Pod or Deployment, step by step, using an example:

Example Scenario: Creating a simple Deployment with one Pod.

1. User Interaction:

You, the user, interact with Kubernetes either through the command-line tool kubectl, a graphical interface, or an automation script.
You submit a YAML file or use a command to create a Deployment. For example:

apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-deployment
spec:
  replicas: 3
  selector:
    matchLabels:
      app: my-app
  template:
    metadata:
      labels:
        app: my-app
    spec:
      containers:
        - name: my-container
          image: nginx
You issue the command: kubectl apply -f my-deployment.yaml.


2. API Server:
Your request is sent to the Kubernetes API Server, which is the control center of the Kubernetes cluster.
The API Server receives your request and performs authentication and authorization checks to ensure you have the necessary permissions to create the resource.

3. etcd (Cluster-Wide Key-Value Store):
The API Server stores the configuration for your Deployment in etcd, which is a distributed, consistent key-value store used to store all cluster data.
etcd acts as the source of truth for the cluster's state.

4. Controller Manager:
The Deployment controller in the Controller Manager watches the etcd store for changes to resources, such as Deployments.
Upon detecting the new Deployment resource, the controller calculates the desired state (in this case, three replicas of your Pod).

5. Scheduler:
The Scheduler is responsible for deciding where to place new Pods.
It evaluates the available worker nodes' resources and constraints and selects the most suitable node for the new Pods.

6. Kubelet (on Selected Node):
The Kubelet on the selected worker node receives instructions from the API Server to create Pods.
It starts the necessary containers on that node, according to the Pod specification.
In our example, it would start three replicas of the nginx container as specified in the Deployment.

7. Container Runtime (e.g., Docker):
The Container Runtime (e.g., Docker) on the node creates and manages the containers as requested by the Kubelet.
It pulls the specified container image (in this case, nginx) from a container registry if it's not already present on the node.

8. Running Pods:
After containers are created, your Pods are up and running on the selected node.

9. Service Discovery (Optional):
If you have defined a Service resource to expose your Deployment, Kubernetes sets up the necessary networking rules to enable service discovery and load balancing for your Pods.

10. Cluster Networking (e.g., CNI):
- If you're using a network plugin like the Container Network Interface (CNI), it ensures that Pods can communicate with each other across nodes in the cluster.

11. User Access:
- Your deployed application is now accessible according to the service type you defined (ClusterIP, NodePort, LoadBalancer, etc.) and any networking configurations you've set up.

In summary, when you create a Pod or Deployment in Kubernetes, your request goes through a series of components, including the API Server, etcd, Controller Manager, Scheduler, Kubelet, and Container Runtime, to bring your application to life. Kubernetes manages the entire lifecycle of your application, ensuring that it runs as desired and is highly available.

-----------------------------------------------------------------------------

what is scheduling, preemption and eviction in k8s

In Kubernetes (often abbreviated as K8s), scheduling, preemption, and eviction are key concepts related to the management and allocation of resources for containerized applications running in a cluster. Let's break down these terms:

Scheduling:

Definition: Scheduling in Kubernetes refers to the process of assigning pods (groups of one or more containers) to nodes (individual machines in a cluster) based on resource requirements, constraints, and various policies.
How it works: When you create a pod or deploy an application in Kubernetes, the Kubernetes Scheduler component decides where to place the pod within the cluster. It considers factors like available resources, node conditions, and user-defined constraints to make an informed placement decision.
Importance: Effective scheduling ensures that pods are distributed optimally across the cluster, avoiding resource bottlenecks and ensuring high availability and efficient resource utilization.

--
Preemption:

Definition: Preemption is the process of reclaiming resources from lower-priority pods to allocate them to higher-priority pods when the cluster is under resource constraints.
How it works: Kubernetes allows you to specify priority and resource requirements for pods. When resources become scarce, the Kubernetes Preemption mechanism identifies lower-priority pods that can be terminated (evicted) to free up resources for higher-priority pods.
Importance: Preemption helps maintain the health and performance of critical applications in resource-constrained environments by ensuring that high-priority workloads get the resources they need.

---
Eviction:

Definition: Eviction is the process of forcibly removing a pod from a node to free up resources. This can happen due to factors like resource constraints, node failures, or administrative actions.
How it works: When a node is under resource pressure or a pod is violating resource limits, Kubernetes may decide to evict the pod. The evicted pod is rescheduled onto another node if possible.
Importance: Eviction helps maintain cluster stability and prevents resource exhaustion by removing pods that are causing issues. It allows Kubernetes to self-heal and ensure that nodes remain functional.

---
In summary:

Scheduling is about placing pods on nodes initially.

Preemption is the process of moving pods to different nodes to free up resources for higher-priority pods.

Eviction is forcibly removing pods from nodes to resolve resource issues or node failures.

These mechanisms work together to ensure efficient and resilient resource management within a Kubernetes cluster, enabling applications to run smoothly in a dynamic and multi-tenant environment.

----------------------------------------------------------------------

task 9 = 

apiVersion: v1
kind: Pod
metadata:
  name: two-containers-pod
spec:
  containers:
  - name: container1
    image: busybox
    command: ["sh", "-c", "echo hello; sleep 3600"]
  - name: container2
    image: busybox
    command: ["sh", "-c", "sleep 3600"]


# Get the name of the pod
pod_name=$(kubectl get pods -l <your_label_selector> -o jsonpath="{.items[0].metadata.name}")

# Connect to the second container in the pod
kubectl exec -it $pod_name -c container2 -- sh

# Now, you are inside the second container, and you can run 'ls'
ls
exit  # To exit from the second container

-----------------------------------------------------------------------

task 10 = 

apiVersion: v1
kind: Pod
metadata:
  name: nginx-busybox-pod
spec:
  containers:
  - name: nginx-container
    image: nginx
    ports:
    - containerPort: 80
    volumeMounts:
    - name: shared-data
      mountPath: /usr/share/nginx/html
  initContainers:
  - name: busybox-init
    image: busybox
    command: ["wget", "-O", "/work-dir/index.html", "http://neverssl.com/online"]
    volumeMounts:
    - name: shared-data
      mountPath: /work-dir
  volumes:
  - name: shared-data
    emptyDir: {}


# Get the IP of the nginx-busybox-pod
pod_ip=$(kubectl get pod nginx-busybox-pod -o=jsonpath='{.status.podIP}')

# Create a busybox pod and run wget to fetch the content from the nginx-busybox-pod
kubectl run busybox-pod --image=busybox --rm --restart=Never -- wget -O- $pod_ip


----------------------------------------------------------------------------------

task 11 = 

apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: pod-log-reader
rules:
- apiGroups: [""]
  resources: ["pods", "pods/log"]
  verbs: ["get", "list", "watch"]


In this configuration:

apiVersion specifies the API version for Role creation.

kind defines the type of resource being created, which is a Role.

metadata contains metadata for the Role, including its name.

rules specify the permissions allowed by this Role.

apiGroups: [""] refers to the core API group, which includes resources like pods.

resources: ["pods", "pods/log"] specifies the resources to which these permissions apply. "pods" allows users to interact with pods, and "pods/log" allows them to access container logs.

verbs: ["get", "list", "watch"] are the allowed actions. Users with this Role will be able to get, list, and watch pods and access their container logs.

---

To bind a Role to specific users or groups in Kubernetes, you need to create a RoleBinding or ClusterRoleBinding resource. Here's how you can do it:

Create a RoleBinding or ClusterRoleBinding:

Use a RoleBinding to bind the Role to users or service accounts within a specific namespace.
Use a ClusterRoleBinding to bind the Role to users or service accounts across the entire cluster.


apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: user-pod-log-reader
  namespace: <namespace-name>
subjects:
- kind: User
  name: <username>
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: Role
  name: pod-log-reader
  apiGroup: rbac.authorization.k8s.io

Replace <namespace-name> with the namespace where you want to bind the Role and <username> with the actual username you want to grant these permissions to.

-------------------------------------------------------------------------------

task 12 = 

apiVersion: v1
kind: Pod
metadata:
  name: red
spec:
  initContainers:
  - name: busybox-init
    image: busybox
    command: ["sleep", "20"]
  containers:
  - name: nginx-container
    image: nginx

--------------------------------------------------------------------

task 13 = 

apiVersion: batch/v1
kind: CronJob
metadata:
  name: hello-cronjob
spec:
  schedule: "*/1 * * * *"
  jobTemplate:
    spec:
      activeDeadlineSeconds: 17
      template:
        spec:
          containers:
          - name: busybox-container
            image: busybox
            command:
            - /bin/sh
            - -c
            - "date; echo Hello from the Kubernetes cluster"
          restartPolicy: OnFailure


In this configuration:

schedule: "*/1 * * * *" specifies that the CronJob should run every minute.

activeDeadlineSeconds: 17 sets the maximum duration in seconds that the job is allowed to run. If the job exceeds this time limit, it will be terminated.

The command section within the busybox-container specifies the command to run, which prints the date and a message to standard output.

restartPolicy: OnFailure indicates that the Pod should be restarted if it fails (which includes exceeding the active deadline), ensuring that it continues to run in subsequent cron executions.

-----------------------------------------------------------------------------

task 14 = 

1) Create a Deployment and Service:
Create a simple Deployment and Service. Here's an example YAML configuration:

apiVersion: apps/v1
kind: Deployment
metadata:
  name: foo-deployment
spec:
  replicas: 2
  selector:
    matchLabels:
      app: foo
  template:
    metadata:
      labels:
        app: foo
    spec:
      containers:
      - name: foo-container
        image: nginx

---

apiVersion: v1
kind: Service
metadata:
  name: foo-service
spec:
  selector:
    app: foo
  ports:
  - protocol: TCP
    port: 80
    targetPort: 80


2) Create a Busybox Pod:
Create a temporary Busybox pod for testing:

apiVersion: v1
kind: Pod
metadata:
  name: busybox-pod
spec:
  containers:
  - name: busybox-container
    image: busybox
    command: ["/bin/sh"]
    args: ["-c", "while true; do wget -q -O- http://foo-service; sleep 1; done"]

3) Verify Different Hostnames:
Run the Busybox pod, and it will continuously make wget requests to the "foo-service" URL. You can check the output of the pod to see different hostnames:

kubectl logs busybox-pod

--------------------------------------------------------------------